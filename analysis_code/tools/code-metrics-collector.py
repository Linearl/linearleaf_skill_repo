#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»£ç æŒ‡æ ‡æ”¶é›†å·¥å…· (Agent Skills ç‰ˆæœ¬)

ç”¨äºè‡ªåŠ¨æ”¶é›† Python é¡¹ç›®çš„åŸºç¡€ä»£ç æŒ‡æ ‡ï¼Œæ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼

ä½¿ç”¨æ–¹æ³•:
    python code-metrics-collector.py --project-path /path/to/project
    python code-metrics-collector.py --project-path /path/to/project --output metrics.json
    python code-metrics-collector.py --project-path /path/to/project --format markdown

è¾“å‡ºæ ¼å¼:
    - json: ç»“æ„åŒ– JSON æ•°æ® (é»˜è®¤)
    - markdown: Markdown æ ¼å¼çš„æŠ¥å‘Š
    - summary: ç®€æ´çš„æ§åˆ¶å°æ‘˜è¦
"""

import os
import ast
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime


@dataclass
class FileMetrics:
    """å•ä¸ªæ–‡ä»¶çš„æŒ‡æ ‡"""

    file_path: str
    lines_of_code: int
    blank_lines: int
    comment_lines: int
    function_count: int
    class_count: int
    import_count: int
    max_complexity: int
    avg_complexity: float
    long_functions: List[str]  # è¶…è¿‡50è¡Œçš„å‡½æ•°å


@dataclass
class ProjectMetrics:
    """é¡¹ç›®æ•´ä½“æŒ‡æ ‡"""

    project_path: str
    analysis_time: str
    total_files: int
    python_files: int
    total_lines: int
    code_lines: int
    blank_lines: int
    comment_lines: int
    total_functions: int
    total_classes: int
    total_imports: int
    avg_file_length: float
    max_file_length: int
    complexity_distribution: Dict[str, int]
    quality_score: float  # 0-100 çš„è´¨é‡è¯„åˆ†
    issues: List[Dict[str, Any]]  # å‘ç°çš„é—®é¢˜


class ComplexityAnalyzer(ast.NodeVisitor):
    """è®¡ç®—åœˆå¤æ‚åº¦çš„ASTè®¿é—®å™¨"""

    def __init__(self):
        self.complexity = 1  # åŸºç¡€å¤æ‚åº¦ä¸º1

    def visit_If(self, node):
        self.complexity += 1
        self.generic_visit(node)

    def visit_While(self, node):
        self.complexity += 1
        self.generic_visit(node)

    def visit_For(self, node):
        self.complexity += 1
        self.generic_visit(node)

    def visit_Try(self, node):
        self.complexity += 1
        self.generic_visit(node)

    def visit_ExceptHandler(self, node):
        self.complexity += 1
        self.generic_visit(node)

    def visit_With(self, node):
        self.complexity += 1
        self.generic_visit(node)

    def visit_BoolOp(self, node):
        # and/or æ“ä½œå¢åŠ å¤æ‚åº¦
        self.complexity += len(node.values) - 1
        self.generic_visit(node)


class CodeMetricsCollector:
    """ä»£ç æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self, verbose: bool = False):
        self.logger = self._setup_logger(verbose)
        self.issues = []

    def _setup_logger(self, verbose: bool) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG if verbose else logging.INFO)

        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def analyze_file(self, file_path: Path) -> Optional[FileMetrics]:
        """åˆ†æå•ä¸ªPythonæ–‡ä»¶"""
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()

            lines = content.split("\n")
            total_lines = len(lines)

            # ç»Ÿè®¡ä»£ç è¡Œã€ç©ºè¡Œã€æ³¨é‡Šè¡Œ
            code_lines = 0
            blank_lines = 0
            comment_lines = 0

            for line in lines:
                stripped = line.strip()
                if not stripped:
                    blank_lines += 1
                elif stripped.startswith("#"):
                    comment_lines += 1
                else:
                    code_lines += 1

            # è§£æAST
            try:
                tree = ast.parse(content)
            except SyntaxError as e:
                self.logger.warning(f"è¯­æ³•é”™è¯¯ {file_path}: {e}")
                self.issues.append(
                    {
                        "type": "syntax_error",
                        "file": str(file_path),
                        "message": str(e),
                        "severity": "high",
                    }
                )
                return None

            # ç»Ÿè®¡å‡½æ•°ã€ç±»ã€å¯¼å…¥
            function_count = 0
            class_count = 0
            import_count = 0
            complexities = []
            long_functions = []

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    function_count += 1
                    # è®¡ç®—å‡½æ•°å¤æ‚åº¦
                    analyzer = ComplexityAnalyzer()
                    analyzer.visit(node)
                    complexities.append(analyzer.complexity)

                    # æ£€æŸ¥å‡½æ•°é•¿åº¦
                    func_lines = (
                        node.end_lineno - node.lineno
                        if hasattr(node, "end_lineno")
                        else 0
                    )
                    if func_lines > 50:
                        long_functions.append(node.name)
                        self.issues.append(
                            {
                                "type": "long_function",
                                "file": str(file_path),
                                "function": node.name,
                                "lines": func_lines,
                                "severity": "medium",
                            }
                        )

                    # æ£€æŸ¥é«˜å¤æ‚åº¦
                    if analyzer.complexity > 10:
                        self.issues.append(
                            {
                                "type": "high_complexity",
                                "file": str(file_path),
                                "function": node.name,
                                "complexity": analyzer.complexity,
                                "severity": (
                                    "medium" if analyzer.complexity <= 20 else "high"
                                ),
                            }
                        )

                elif isinstance(node, ast.ClassDef):
                    class_count += 1
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    import_count += 1

            # æ£€æŸ¥æ–‡ä»¶è¿‡é•¿
            if total_lines > 500:
                self.issues.append(
                    {
                        "type": "long_file",
                        "file": str(file_path),
                        "lines": total_lines,
                        "severity": "low",
                    }
                )

            # è®¡ç®—å¤æ‚åº¦ç»Ÿè®¡
            max_complexity = max(complexities) if complexities else 0
            avg_complexity = (
                sum(complexities) / len(complexities) if complexities else 0
            )

            return FileMetrics(
                file_path=str(file_path),
                lines_of_code=code_lines,
                blank_lines=blank_lines,
                comment_lines=comment_lines,
                function_count=function_count,
                class_count=class_count,
                import_count=import_count,
                max_complexity=max_complexity,
                avg_complexity=round(avg_complexity, 2),
                long_functions=long_functions,
            )

        except Exception as e:
            self.logger.error(f"åˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def collect_project_metrics(
        self, project_path: Path, exclude_patterns: List[str] = None
    ) -> ProjectMetrics:
        """æ”¶é›†é¡¹ç›®çº§åˆ«çš„æŒ‡æ ‡"""
        python_files = []
        exclude_patterns = exclude_patterns or [
            ".venv",
            "__pycache__",
            ".git",
            "node_modules",
        ]

        # æŸ¥æ‰¾æ‰€æœ‰Pythonæ–‡ä»¶
        for py_file in project_path.rglob("*.py"):
            # æ’é™¤ç‰¹å®šç›®å½•
            if not any(pattern in str(py_file) for pattern in exclude_patterns):
                python_files.append(py_file)

        self.logger.info(f"æ‰¾åˆ° {len(python_files)} ä¸ªPythonæ–‡ä»¶")

        # åˆ†ææ¯ä¸ªæ–‡ä»¶
        file_metrics = []
        for py_file in python_files:
            metrics = self.analyze_file(py_file)
            if metrics:
                file_metrics.append(metrics)

        # è®¡ç®—é¡¹ç›®çº§åˆ«æŒ‡æ ‡
        total_files = len(file_metrics)
        total_lines = sum(
            m.lines_of_code + m.blank_lines + m.comment_lines for m in file_metrics
        )
        code_lines = sum(m.lines_of_code for m in file_metrics)
        blank_lines = sum(m.blank_lines for m in file_metrics)
        comment_lines = sum(m.comment_lines for m in file_metrics)
        total_functions = sum(m.function_count for m in file_metrics)
        total_classes = sum(m.class_count for m in file_metrics)
        total_imports = sum(m.import_count for m in file_metrics)

        # è®¡ç®—å¹³å‡å€¼
        avg_file_length = total_lines / total_files if total_files > 0 else 0
        max_file_length = (
            max(
                (m.lines_of_code + m.blank_lines + m.comment_lines)
                for m in file_metrics
            )
            if file_metrics
            else 0
        )

        # å¤æ‚åº¦åˆ†å¸ƒ
        all_complexities = [
            m.max_complexity for m in file_metrics if m.max_complexity > 0
        ]

        complexity_distribution = {
            "low": sum(1 for c in all_complexities if 1 <= c <= 10),
            "medium": sum(1 for c in all_complexities if 11 <= c <= 20),
            "high": sum(1 for c in all_complexities if 21 <= c <= 50),
            "very_high": sum(1 for c in all_complexities if c > 50),
        }

        # è®¡ç®—è´¨é‡è¯„åˆ† (ç®€åŒ–ç‰ˆ)
        quality_score = self._calculate_quality_score(
            code_lines, comment_lines, complexity_distribution, total_functions
        )

        return ProjectMetrics(
            project_path=str(project_path),
            analysis_time=datetime.now().isoformat(),
            total_files=total_files,
            python_files=len(python_files),
            total_lines=total_lines,
            code_lines=code_lines,
            blank_lines=blank_lines,
            comment_lines=comment_lines,
            total_functions=total_functions,
            total_classes=total_classes,
            total_imports=total_imports,
            avg_file_length=round(avg_file_length, 2),
            max_file_length=max_file_length,
            complexity_distribution=complexity_distribution,
            quality_score=quality_score,
            issues=self.issues,
        )

    def _calculate_quality_score(
        self,
        code_lines: int,
        comment_lines: int,
        complexity_dist: Dict[str, int],
        total_functions: int,
    ) -> float:
        """è®¡ç®—ä»£ç è´¨é‡è¯„åˆ† (0-100)"""
        score = 100.0

        # æ³¨é‡Šè¦†ç›–ç‡è¯„åˆ† (ç›®æ ‡ > 15%)
        comment_ratio = comment_lines / code_lines if code_lines > 0 else 0
        if comment_ratio < 0.10:
            score -= 15
        elif comment_ratio < 0.15:
            score -= 5

        # å¤æ‚åº¦è¯„åˆ†
        if total_functions > 0:
            high_complexity_ratio = (
                complexity_dist.get("high", 0) + complexity_dist.get("very_high", 0)
            ) / total_functions
            if high_complexity_ratio > 0.2:
                score -= 20
            elif high_complexity_ratio > 0.1:
                score -= 10

        # é—®é¢˜æ•°é‡æ‰£åˆ†
        high_issues = sum(1 for i in self.issues if i.get("severity") == "high")
        medium_issues = sum(1 for i in self.issues if i.get("severity") == "medium")
        score -= high_issues * 5
        score -= medium_issues * 2

        return max(0, min(100, round(score, 1)))

    def save_json(self, metrics: ProjectMetrics, output_path: Path):
        """ä¿å­˜æŒ‡æ ‡åˆ°JSONæ–‡ä»¶"""
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(asdict(metrics), f, indent=2, ensure_ascii=False)
        self.logger.info(f"JSONæŒ‡æ ‡å·²ä¿å­˜åˆ°: {output_path}")

    def save_markdown(self, metrics: ProjectMetrics, output_path: Path):
        """ä¿å­˜æŒ‡æ ‡åˆ°Markdownæ–‡ä»¶"""
        md_content = self._generate_markdown_report(metrics)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(md_content)
        self.logger.info(f"MarkdownæŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")

    def _generate_markdown_report(self, metrics: ProjectMetrics) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        quality_emoji = (
            "ğŸŸ¢"
            if metrics.quality_score >= 80
            else "ğŸŸ¡" if metrics.quality_score >= 60 else "ğŸ”´"
        )

        report = f"""# ä»£ç æŒ‡æ ‡åˆ†ææŠ¥å‘Š

> ç”Ÿæˆæ—¶é—´: {metrics.analysis_time}  
> é¡¹ç›®è·¯å¾„: `{metrics.project_path}`

## ğŸ“Š æ•´ä½“è¯„ä¼°

| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **è´¨é‡è¯„åˆ†** | {quality_emoji} {metrics.quality_score}/100 | ç»¼åˆè´¨é‡è¯„åˆ† |
| Pythonæ–‡ä»¶æ•° | {metrics.python_files} | åˆ†æçš„æ–‡ä»¶æ•°é‡ |
| æ€»ä»£ç è¡Œæ•° | {metrics.code_lines} | ä¸å«ç©ºè¡Œå’Œæ³¨é‡Š |
| å‡½æ•°æ•°é‡ | {metrics.total_functions} | - |
| ç±»æ•°é‡ | {metrics.total_classes} | - |

## ğŸ“ˆ ä»£ç ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ€»è¡Œæ•° | {metrics.total_lines} |
| ä»£ç è¡Œ | {metrics.code_lines} |
| æ³¨é‡Šè¡Œ | {metrics.comment_lines} |
| ç©ºç™½è¡Œ | {metrics.blank_lines} |
| å¹³å‡æ–‡ä»¶é•¿åº¦ | {metrics.avg_file_length} è¡Œ |
| æœ€å¤§æ–‡ä»¶é•¿åº¦ | {metrics.max_file_length} è¡Œ |

## ğŸ” å¤æ‚åº¦åˆ†å¸ƒ

| å¤æ‚åº¦ç­‰çº§ | å‡½æ•°æ•°é‡ | è¯´æ˜ |
|------------|----------|------|
| ğŸŸ¢ ä½ (1-10) | {metrics.complexity_distribution['low']} | è‰¯å¥½ |
| ğŸŸ¡ ä¸­ (11-20) | {metrics.complexity_distribution['medium']} | å¯æ¥å— |
| ğŸŸ  é«˜ (21-50) | {metrics.complexity_distribution['high']} | éœ€è¦å…³æ³¨ |
| ğŸ”´ æé«˜ (>50) | {metrics.complexity_distribution['very_high']} | éœ€è¦é‡æ„ |

## âš ï¸ å‘ç°çš„é—®é¢˜

"""
        if metrics.issues:
            report += "| ä¸¥é‡åº¦ | ç±»å‹ | æ–‡ä»¶ | è¯¦æƒ… |\n"
            report += "|--------|------|------|------|\n"
            for issue in metrics.issues[:20]:  # æœ€å¤šæ˜¾ç¤º20ä¸ª
                severity_emoji = (
                    "ğŸ”´"
                    if issue["severity"] == "high"
                    else "ğŸŸ¡" if issue["severity"] == "medium" else "ğŸŸ¢"
                )
                detail = (
                    issue.get("function", "")
                    or issue.get("message", "")
                    or f"{issue.get('lines', '')} è¡Œ"
                )
                report += f"| {severity_emoji} {issue['severity']} | {issue['type']} | `{Path(issue['file']).name}` | {detail} |\n"

            if len(metrics.issues) > 20:
                report += f"\n*è¿˜æœ‰ {len(metrics.issues) - 20} ä¸ªé—®é¢˜æœªæ˜¾ç¤º...*\n"
        else:
            report += "âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜\n"

        report += f"""
---

*ç”± analysis_code skill çš„ code-metrics-collector å·¥å…·ç”Ÿæˆ*
"""
        return report

    def print_summary(self, metrics: ProjectMetrics):
        """æ‰“å°æ§åˆ¶å°æ‘˜è¦"""
        quality_indicator = (
            "ğŸŸ¢"
            if metrics.quality_score >= 80
            else "ğŸŸ¡" if metrics.quality_score >= 60 else "ğŸ”´"
        )

        print(f"\n{'='*50}")
        print(f"ğŸ“Š ä»£ç åˆ†ææ‘˜è¦")
        print(f"{'='*50}")
        print(f"é¡¹ç›®è·¯å¾„: {metrics.project_path}")
        print(f"åˆ†ææ—¶é—´: {metrics.analysis_time}")
        print(f"è´¨é‡è¯„åˆ†: {quality_indicator} {metrics.quality_score}/100")
        print(f"{'='*50}")
        print(f"Pythonæ–‡ä»¶: {metrics.python_files}")
        print(f"ä»£ç è¡Œæ•°: {metrics.code_lines}")
        print(f"å‡½æ•°æ•°é‡: {metrics.total_functions}")
        print(f"ç±»æ•°é‡: {metrics.total_classes}")
        print(f"å¹³å‡æ–‡ä»¶é•¿åº¦: {metrics.avg_file_length} è¡Œ")
        print(f"{'='*50}")
        print(f"å¤æ‚åº¦åˆ†å¸ƒ:")
        print(f"  ğŸŸ¢ ä½: {metrics.complexity_distribution['low']}")
        print(f"  ğŸŸ¡ ä¸­: {metrics.complexity_distribution['medium']}")
        print(f"  ğŸŸ  é«˜: {metrics.complexity_distribution['high']}")
        print(f"  ğŸ”´ æé«˜: {metrics.complexity_distribution['very_high']}")
        print(f"{'='*50}")

        if metrics.issues:
            high_count = sum(1 for i in metrics.issues if i["severity"] == "high")
            medium_count = sum(1 for i in metrics.issues if i["severity"] == "medium")
            low_count = sum(1 for i in metrics.issues if i["severity"] == "low")
            print(f"å‘ç°é—®é¢˜: ğŸ”´é«˜{high_count} ğŸŸ¡ä¸­{medium_count} ğŸŸ¢ä½{low_count}")
        else:
            print("å‘ç°é—®é¢˜: âœ… æ— ")
        print(f"{'='*50}\n")


def main():
    parser = argparse.ArgumentParser(
        description="Pythoné¡¹ç›®ä»£ç æŒ‡æ ‡æ”¶é›†å·¥å…· (Agent Skills ç‰ˆæœ¬)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s --project-path ./my_project
  %(prog)s --project-path ./my_project --output metrics.json
  %(prog)s --project-path ./my_project --format markdown --output report.md
        """,
    )
    parser.add_argument(
        "--project-path", "-p", type=str, required=True, help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„"
    )
    parser.add_argument("--output", "-o", type=str, default=None, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--format",
        "-f",
        type=str,
        choices=["json", "markdown", "summary"],
        default="summary",
        help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: summary)",
    )
    parser.add_argument(
        "--exclude",
        "-e",
        type=str,
        nargs="*",
        default=[".venv", "__pycache__", ".git", "node_modules"],
        help="æ’é™¤çš„ç›®å½•æ¨¡å¼",
    )
    parser.add_argument("--verbose", "-v", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—")

    args = parser.parse_args()

    # éªŒè¯é¡¹ç›®è·¯å¾„
    project_path = Path(args.project_path)
    if not project_path.exists():
        print(f"âŒ é”™è¯¯: é¡¹ç›®è·¯å¾„ä¸å­˜åœ¨: {project_path}")
        return 1

    if not project_path.is_dir():
        print(f"âŒ é”™è¯¯: é¡¹ç›®è·¯å¾„ä¸æ˜¯ç›®å½•: {project_path}")
        return 1

    # æ”¶é›†æŒ‡æ ‡
    collector = CodeMetricsCollector(verbose=args.verbose)
    metrics = collector.collect_project_metrics(project_path, args.exclude)

    # è¾“å‡ºç»“æœ
    if args.format == "json":
        output_path = Path(args.output) if args.output else Path("metrics.json")
        collector.save_json(metrics, output_path)
        collector.print_summary(metrics)
    elif args.format == "markdown":
        output_path = Path(args.output) if args.output else Path("analysis_report.md")
        collector.save_markdown(metrics, output_path)
        collector.print_summary(metrics)
    else:  # summary
        collector.print_summary(metrics)
        if args.output:
            collector.save_json(metrics, Path(args.output))

    return 0


if __name__ == "__main__":
    exit(main())
